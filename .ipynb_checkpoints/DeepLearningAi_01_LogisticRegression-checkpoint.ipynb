{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Logistic Regression"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "### Binary classification examples"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "A training example consists of input vector $x$ and output boolean $y$:  \n$(\\boldsymbol{x}, y) \\mid \\boldsymbol{x} \\in \\mathbb{R}^{n_x}, y \\in \\lbrace 0, 1 \\rbrace$  \n\n$m$ training examples:  \n$\n\\left\\lbrace\n\\left(\\boldsymbol{x}^{(1)}, y^{(1)}\\right), \n\\left(\\boldsymbol{x}^{(2)}, y^{(2)}\\right),\n\\dots,\n\\left(\\boldsymbol{x}^{(m)}, y^{(m)}\\right)\n\\right\\rbrace\n$  \n\nSo the full training set $\\boldsymbol{X}$ consists of $m$ vectors that are $n$ long giving a $(n×m)$ shaped matrix:  \n$\n\\boldsymbol{X} =\n\\begin{vmatrix}\n\\mid & \\mid & \\mid & \\mid \\\\\n\\boldsymbol{x}^{(1)} &  \\boldsymbol{x}^{(2)} & \\dots & \\boldsymbol{x}^{(m)} \\\\\n\\mid & \\mid & \\mid & \\mid\n\\end{vmatrix}\n$  \n\nThe corresponding $y$ solutions form the following $(1×m)$ matrix:  \n$\n\\boldsymbol{Y} =\n\\left[\ny^{(1)}, y^{(2)}, \\dots, y^{(m)}\n\\right]\n$ \n"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "### Logistic Regression - A Single-Neuron System\nfor solving Binary Classification"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Looking for: $\\hat{y} = P(y=1 \\mid \\boldsymbol{x})$  \nNote: $0 \\le \\hat{y} \\le 1$\n\nLogistic Regression parameters: weights and bias: $\\boldsymbol{w} \\in \\mathbb{R}^{n_x}, b \\in \\mathbb{R}$  \nLogistic Regression output: $$\\hat{y} = \\sigma(\\boldsymbol{w}^{\\intercal}\\boldsymbol{x} + b)$$\n\nWhere: sigmoid fn: $\\sigma(z) = \\frac{1}{1+e^{-z}}$"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Cross-Entropy Loss Function (\"Log Loss\")"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "MSE (Mean Squared Error) $L2 = \\frac{1}{2}(\\hat{y}-y)^2$ is not good because for Logistic Regression it tends to have many local minima.  \n\nInstead we use **Cross-Entropy Loss** which will have one global minimum: $$L(\\hat{y}, y) = -(y\\log\\hat{y} + (1-y)\\log(1-\\hat{y}))$$  \nIntuition: since $y \\in \\lbrace0,1\\rbrace$:  \n- if $y=1: L(\\hat{y}, y) = -\\log\\hat{y}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\nAs $\\hat{y}$ gets further from true 1, $L$ gets exponentially bigger.\n- if $y=0: L(\\hat{y}, y) = -\\log(1-\\hat{y})$ &nbsp;&nbsp;&nbsp;\nAs $\\hat{y}$ gets further from true 0, $L$ gets exponentially bigger.  \n\nWhy is it called *Cross-Entropy*?  \n\n**Entropy** is average amount of information contained in an element of a distribution.  \nInformation can be regarded as deviance from ground zero. It can be measured by comparing a distribution (i.e. the prediction) to an actual fixed outcome. We want to measure how many times uncertainity was halved when replacing the predicted distribution with the actual outcome. This is the number of *bits* of information.  \nE.g. take a predicted discrete distribution $\\hat{p}$ size 2: $\\hat{p}_0 = 0.75$ and $\\hat{p}_1 = 0.25$. Let's say the fixed outcome is $p_0=0$ and $p_1=1$. The uncertainity for case $\\hat{p}_1$ has decreased four-fold, which is 2 bits of information. This can also be calculated as $InfoBits = -\\log_2\\hat{p}_1$. Applying the same calculation to $\\hat{p}_0$ yields 0.415 bits. Taking the weighted average of the two bit counts is the entropy of the distribution: $0.75*0.415+0.25*2 = 0.811$ bits.  \n**Entropy function:**\n$$H(\\hat{p}) = -\\sum_i(\\hat{p}_i\\log_2\\hat{p}_i)$$\n**Cross-Entropy**, however, is the amount of information gained by learning a particular truth in relation to a predicted distribution. It depends on both the predicted distribution $\\hat{p}$ and the learned truth-distribution $p$.\n$$H(p,\\hat{p}) = -\\sum_i(p_i\\log_2\\hat{p}_i)$$"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cross_entropy(y, yhat):\n    return -(y*math.log2(y))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"hee\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}